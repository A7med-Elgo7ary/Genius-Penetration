#!/usr/bin/env python3
"""
LLM Interface for the Exploitation Agent.
This module handles interactions with the Gemini API for exploitation decision-making.
"""

import os
import json
import logging
import subprocess
import time
import sys
from pathlib import Path
import base64
import re

# Add the parent directory to system path to enable imports
sys.path.append(str(Path(__file__).resolve().parent.parent.parent))

from utils.logger import setup_logger
from google.generativeai import configure, GenerativeModel

class ExploitLLMInterface:
    """
    Interface for interacting with the Gemini API for exploitation tasks.
    """
    
    def __init__(self, config=None):
        """
        Initialize the LLM interface with configuration.
        
        Args:
            config (dict): Configuration for the LLM interface
        """
        self.logger = setup_logger("exploit_llm_interface")
        self.logger.info("Initializing Exploitation LLM Interface")
        
        # Default configuration
        self.config = {
            "model": "gemini-2.5-flash-preview-04-17",
            "temperature": 0.2,
            "top_p": 0.95,
            "top_k": 40,
            "max_output_tokens": 8192
        }
        
        # Update with provided config
        if config:
            self.config.update(config)
            
        # Get API key from environment or config
        self.api_key = os.environ.get("GEMINI_API_KEY") or self.config.get("api_key")
        
        if not self.api_key:
            self.logger.error("No Gemini API key provided. Please set GEMINI_API_KEY environment variable or provide it in config.")
            raise ValueError("No Gemini API key provided")
            
        # Initialize Gemini API
        configure(api_key=self.api_key)
        self.model = GenerativeModel(
            model_name=self.config["model"],
            generation_config={
                "temperature": self.config["temperature"],
                "top_p": self.config["top_p"],
                "top_k": self.config["top_k"],
                "max_output_tokens": self.config["max_output_tokens"]
            }
        )
        
        self.logger.info(f"Initialized Gemini model: {self.config['model']}")
        
    def generate_exploitation_plan(self, vuln_analysis_data):
        """
        Generate an exploitation plan based on vulnerability analysis data.
        
        Args:
            vuln_analysis_data (dict): The vulnerability analysis data
            
        Returns:
            dict: A dictionary mapping vulnerability IDs to exploitation details
        """
        self.logger.info("Generating exploitation plan")
        
        # Create prompt for generating exploitation plan
        prompt = self._create_exploitation_plan_prompt(vuln_analysis_data)
        
        try:
            # Generate exploitation plan using Gemini
            response = self.model.generate_content(prompt)
            exploitation_plan_text = response.text
            
            # Extract JSON from the response
            exploitation_plan = self._extract_json_from_response(exploitation_plan_text)
            
            if not exploitation_plan:
                self.logger.error("Failed to parse exploitation plan from LLM response")
                return {}
                
            self.logger.info(f"Generated exploitation plan for {len(exploitation_plan)} vulnerabilities")
            return exploitation_plan
            
        except Exception as e:
            self.logger.error(f"Error generating exploitation plan: {str(e)}")
            return {}
    
    def execute_custom_exploit(self, method, target, params):
        """
        Execute a custom exploit based on LLM guidance.
        
        Args:
            method (str): The custom exploit method or technique
            target (str): The target for the exploit
            params (dict): Parameters for the exploit
            
        Returns:
            dict: Result of the exploit attempt
        """
        self.logger.info(f"Generating custom exploit for {method} against {target}")
        
        # Create prompt for custom exploit
        prompt = self._create_custom_exploit_prompt(method, target, params)
        
        try:
            # Generate custom exploit code using Gemini
            response = self.model.generate_content(prompt)
            exploit_response = response.text
            
            # Extract code blocks from the response
            code_blocks = self._extract_code_blocks(exploit_response)
            
            if not code_blocks:
                self.logger.error("No executable code blocks found in LLM response")
                return {"success": False, "message": "No executable code blocks found in LLM response"}
            
            # Execute the code blocks
            return self._execute_code_blocks(code_blocks, target, params)
            
        except Exception as e:
            self.logger.error(f"Error executing custom exploit: {str(e)}")
            return {"success": False, "message": str(e)}
    
    def _create_exploitation_plan_prompt(self, vuln_analysis_data):
        """
        Create a prompt for generating an exploitation plan.
        
        Args:
            vuln_analysis_data (dict): The vulnerability analysis data
            
        Returns:
            str: The generated prompt
        """
        target = vuln_analysis_data.get("target", "Unknown")
        vulnerabilities = vuln_analysis_data.get("vulnerabilities", [])
        
        # Format vulnerability data for the prompt
        vuln_details = []
        for idx, vuln in enumerate(vulnerabilities):
            vuln_id = vuln.get("id", f"VULN-{idx+1}")
            description = vuln.get("description", "No description")
            cve_id = vuln.get("cve_id", "No CVE")
            cvss_score = vuln.get("cvss_score", "Unknown")
            affected_component = vuln.get("affected_component", "Unknown")
            exploitation_likelihood = vuln.get("exploitation_likelihood", "Unknown")
            known_exploits = vuln.get("known_exploits", [])
            
            # Format known exploits
            exploit_details = []
            for exploit in known_exploits:
                exploit_details.append(f"- {exploit.get('name', 'Unknown')}: {exploit.get('description', 'No description')}")
                
            vuln_details.append(f"""
Vulnerability ID: {vuln_id}
Description: {description}
CVE ID: {cve_id}
CVSS Score: {cvss_score}
Affected Component: {affected_component}
Exploitation Likelihood: {exploitation_likelihood}
Known Exploits:
{chr(10).join(exploit_details) if exploit_details else "- No known exploits"}
""")
        
        # Create the prompt
        prompt = f"""You are an expert penetration tester and exploitation specialist operating as part of the AI-PenTest Exploitation Agent. You are working within a Kali Linux 2025 environment with full access to all standard penetration testing tools.

Your task is to analyze the following vulnerabilities discovered during a vulnerability assessment and develop an exploitation plan. For each vulnerability that you believe can be exploited, specify the appropriate tool, method, target, and any required parameters.

TARGET INFORMATION:
Target: {target}

VULNERABILITIES:
{chr(10).join(vuln_details)}

AVAILABLE EXPLOITATION TOOLS:
1. Metasploit Framework - For various exploits
2. Commix - For command injection exploits
3. Custom exploits - For cases requiring custom scripts or techniques

Based on the above information, create an exploitation plan in JSON format that maps each exploitable vulnerability ID to the exploitation details (tool, method, target, parameters).

The JSON format should be as follows:
```json
{{
  "VULN-ID-1": {{
    "tool": "metasploit",
    "method": "exploit/path/in/metasploit",
    "target": "target_ip_or_url_with_port",
    "params": {{
      "param1": "value1",
      "param2": "value2"
    }}
  }},
  "VULN-ID-2": {{
    "tool": "commix",
    "method": "command_injection",
    "target": "target_url_with_vulnerable_parameter",
    "params": {{
      "param1": "value1",
      "param2": "value2"
    }}
  }}
}}
```

Only include vulnerabilities that you believe can be successfully exploited based on the information provided. For each vulnerability, select the most appropriate and reliable exploitation method.

Provide only the JSON exploitation plan without additional explanations.
"""
        
        return prompt
    
    def _create_custom_exploit_prompt(self, method, target, params):
        """
        Create a prompt for generating a custom exploit.
        
        Args:
            method (str): The custom exploit method or technique
            target (str): The target for the exploit
            params (dict): Parameters for the exploit
            
        Returns:
            str: The generated prompt
        """
        # Format parameters for the prompt
        params_str = "\n".join([f"- {key}: {value}" for key, value in params.items()])
        
        prompt = f"""You are an expert penetration tester and exploitation specialist operating as part of the AI-PenTest Exploitation Agent. You are working within a Kali Linux 2025 environment with full access to all standard penetration testing tools.

Your task is to create a custom exploit script for the following scenario:

EXPLOIT METHOD: {method}
TARGET: {target}
PARAMETERS:
{params_str}

Create a Python script that will exploit the target using the given method and parameters. The script should:

1. Be fully executable within the Kali Linux environment
2. Handle potential errors and provide clear feedback on success or failure
3. Return a standardized result in this format: {{"success": true/false, "message": "details", "session_id": "id_if_applicable"}}

Provide only the Python code without additional explanations. The code should be ready to execute and properly formatted.
"""
        
        return prompt
    
    def _extract_json_from_response(self, response_text):
        """
        Extract JSON data from LLM response text.
        
        Args:
            response_text (str): The LLM response text
            
        Returns:
            dict: The extracted JSON data or empty dict if extraction fails
        """
        try:
            # Try to extract JSON between code blocks first
            json_match = re.search(r'```(?:json)?\s*([\s\S]*?)\s*```', response_text)
            if json_match:
                json_str = json_match.group(1)
                return json.loads(json_str)
            
            # If that fails, try to parse the entire response as JSON
            return json.loads(response_text)
        except json.JSONDecodeError:
            self.logger.error("Failed to parse JSON from LLM response")
            self.logger.debug(f"Response text: {response_text}")
            return {}
    
    def _extract_code_blocks(self, response_text):
        """
        Extract executable code blocks from LLM response text.
        
        Args:
            response_text (str): The LLM response text
            
        Returns:
            list: List of code blocks
        """
        # Extract code blocks between triple backticks
        code_blocks = re.findall(r'```(?:python)?\s*([\s\S]*?)\s*```', response_text)
        
        # If no code blocks with backticks, try to extract the entire response
        if not code_blocks and not response_text.strip().startswith('```'):
            code_blocks = [response_text]
            
        return code_blocks
    
    def _execute_code_blocks(self, code_blocks, target, params):
        """
        Execute extracted code blocks as a Python script.
        
        Args:
            code_blocks (list): List of code blocks to execute
            target (str): The target for the exploit
            params (dict): Parameters for the exploit
            
        Returns:
            dict: Result of the execution
        """
        # Combine all code blocks
        full_code = "\n\n".join(code_blocks)
        
        # Create temporary script file
        script_path = os.path.join(os.path.dirname(__file__), "temp_exploit.py")
        
        try:
            with open(script_path, 'w') as f:
                f.write(full_code)
            
            # Make the script executable
            os.chmod(script_path, 0o755)
            
            # Execute the script
            self.logger.info(f"Executing custom exploit script against {target}")
            
            # Convert params to command line arguments
            args = []
            for key, value in params.items():
                args.extend([f"--{key}", str(value)])
                
            cmd = [script_path, target] + args
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            stdout, stderr = process.communicate(timeout=300)  # 5-minute timeout
            
            if process.returncode != 0:
                self.logger.error(f"Exploit script returned non-zero exit code: {process.returncode}")
                self.logger.debug(f"Script stderr: {stderr}")
                return {
                    "success": False, 
                    "message": f"Exploit failed with exit code {process.returncode}: {stderr}"
                }
            
            # Try to parse the output as JSON
            try:
                result = json.loads(stdout)
                return result
            except json.JSONDecodeError:
                # If not valid JSON, create a simple result
                return {
                    "success": True,
                    "message": stdout.strip(),
                    "raw_output": base64.b64encode(stdout.encode()).decode()
                }
                
        except subprocess.TimeoutExpired:
            self.logger.error("Exploit script execution timed out after 5 minutes")
            return {"success": False, "message": "Exploit script execution timed out"}
        except Exception as e:
            self.logger.error(f"Error executing exploit script: {str(e)}")
            return {"success": False, "message": str(e)}
        finally:
            # Clean up the temporary script
            if os.path.exists(script_path):
                try:
                    os.remove(script_path)
                except:
                    pass