#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
LLM Interface for Vulnerability Analysis Agent

This module handles interactions with the Gemini API for vulnerability analysis,
including scan result summarization, vulnerability analysis, and recommendation generation.
"""

import os
import json
import logging
import google.generativeai as genai
from google.generativeai.types import GenerationConfig
from typing import Dict, List, Any, Optional

# Configure logging
logger = logging.getLogger("vulnerability_analysis_agent.llm")

class LLMInterface:
    """Interface for interacting with the Gemini API for vulnerability analysis"""
    
    def __init__(self, model_name="gemini-2.5-flash-preview-04-17"):
        """
        Initialize the LLM Interface
        
        Args:
            model_name (str): Name of the Gemini model to use
        """
        self.model_name = model_name
        
        # Get API key from environment variable
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            logger.warning("GEMINI_API_KEY environment variable not set. LLM functionality will be limited.")
        
        try:
            # Configure the Gemini API
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel(model_name)
            logger.info(f"Initialized Gemini API with model: {model_name}")
        except Exception as e:
            logger.error(f"Failed to initialize Gemini API: {str(e)}")
            self.model = None
    
    def _extract_json_from_response(self, response: Optional[str]) -> Optional[Any]:
        """
        Attempt to extract a JSON object from a string response
        
        Args:
            response (str): String that might contain a JSON object
            
        Returns:
            Any: Parsed JSON object or None if extraction failed
        """
        if not response:
            return None
        
        # Try to extract JSON from response by looking for balanced braces
        try:
            # Check if the response is already valid JSON
            return json.loads(response)
        except json.JSONDecodeError:
            # Try to find the JSON portion of the response
            try:
                # Find the first opening brace
                start_idx = response.find('{')
                # Find the last closing brace
                end_idx = response.rfind('}')
                
                if start_idx != -1 and end_idx != -1 and start_idx < end_idx:
                    json_str = response[start_idx:end_idx+1]
                    return json.loads(json_str)
                
                # Also try for arrays
                start_idx = response.find('[')
                end_idx = response.rfind(']')
                
                if start_idx != -1 and end_idx != -1 and start_idx < end_idx:
                    json_str = response[start_idx:end_idx+1]
                    return json.loads(json_str)
                
                return None
            except (json.JSONDecodeError, ValueError):
                logger.error("Failed to extract JSON from response")
                return None
                
    def _call_model(self, prompt: str, temperature: float = 0.2) -> Optional[str]:
        """
        Call the Gemini model with the given prompt
        
        Args:
            prompt (str): The prompt to send to the model
            temperature (float): The temperature parameter for generation
            
        Returns:
            str: The model's response or None if an error occurred
        """
        if not self.model:
            logger.warning("Gemini model not initialized. Cannot call API.")
            return None
        
        try:
            # Add specific instructions for JSON formatting at the beginning of the prompt
            formatted_prompt = f"""You are an AI assistant specialized in cybersecurity analysis.
            Your response MUST be in valid JSON format. Structure your JSON according to the specifications below.
            Do not include any explanations, markdown, or text outside of the JSON object itself.
            
            {prompt}"""
            
            response = self.model.generate_content(
                contents=formatted_prompt,
                generation_config=GenerationConfig(
                    temperature=temperature,
                    response_mime_type="application/json"
                )
            )
            return response.text
        except Exception as e:
            logger.error(f"Error calling Gemini API: {str(e)}")
            return None
    
    def summarize_scan_results(self, scan_results: Dict) -> Dict:
        """
        Summarize the vulnerability scan results using the Gemini API
        
        Args:
            scan_results (dict): The scan results to summarize
            
        Returns:
            dict: A summary of the scan results including overview and statistics
        """
        logger.info("Summarizing vulnerability scan results")
        
        # Convert scan results to a string representation for the prompt
        scan_results_str = json.dumps(scan_results, indent=2)
        
        # Create a prompt for the LLM to summarize the scan results
        prompt = f"""
        You are a cybersecurity expert analyzing vulnerability scan results. 
        Below are the results from a vulnerability scan. Please provide a comprehensive summary including:
        
        1. An overview of the scan (what was scanned, when, tools used)
        2. Total number of vulnerabilities found
        3. Distribution by severity (high, medium, low, unknown)
        4. Distribution by vulnerability type (e.g., XSS, SQLi, etc.)
        5. Most critical vulnerabilities to address first
        
        Format your response as a JSON object with the following structure:
        {{
            "overview": "Comprehensive overview here...",
            "total_vulnerabilities": number,
            "severity_distribution": {{
                "high": number,
                "medium": number,
                "low": number,
                "unknown": number
            }},
            "vulnerability_types": {{
                "type1": number,
                "type2": number,
                ...
            }},
            "critical_findings": [
                {{
                    "title": "Brief title of the vulnerability",
                    "severity": "High/Medium/Low",
                    "description": "Brief description"
                }},
                ...
            ]
        }}
        
        Here are the scan results:
        
        {scan_results_str}
        """
        
        # Call the model and parse the response
        response = self._call_model(prompt)
        
        if not response:
            logger.warning("Failed to get summary from LLM. Returning empty summary.")
            return {
                "overview": "Failed to generate summary.",
                "total_vulnerabilities": 0,
                "severity_distribution": {},
                "vulnerability_types": {},
                "critical_findings": []
            }
        
        try:
            # Parse the JSON response
            summary = json.loads(response)
            logger.info("Successfully generated scan summary")
            return summary
        except json.JSONDecodeError:
            # If that fails, try to extract JSON from the response
            try:
                # Find the first '{' and last '}' to extract the JSON object
                start_idx = response.find('{')
                end_idx = response.rfind('}')
                
                if start_idx != -1 and end_idx != -1 and start_idx < end_idx:
                    json_str = response[start_idx:end_idx+1]
                    summary = json.loads(json_str)
                    logger.info("Successfully extracted JSON from scan summary response")
                    return summary
                else:
                    logger.error("Failed to extract JSON from response. Returning text response.")
                    return {
                        "overview": response[:1000],  # Truncate if too long
                        "total_vulnerabilities": 0,
                        "severity_distribution": {},
                        "vulnerability_types": {},
                        "critical_findings": []
                    }
            except json.JSONDecodeError:
                logger.error("Failed to parse LLM response as JSON. Returning text response.")
                return {
                    "overview": response[:1000],  # Truncate if too long
                    "total_vulnerabilities": 0,
                    "severity_distribution": {},
                    "vulnerability_types": {},
                    "critical_findings": []
                }
    
    def enhance_vulnerability_lookup(self, description: str) -> List[Dict]:
        """
        Use the LLM to enhance vulnerability lookup when direct CVE matching fails
        
        Args:
            description (str): The vulnerability description to analyze
            
        Returns:
            list: List of potential CVE matches with details
        """
        logger.info("Using LLM to enhance vulnerability lookup")
        
        prompt = f"""
        You are a cybersecurity expert. Based on the following vulnerability description, 
        identify the most likely CVE(s) that match this vulnerability. 
        
        Vulnerability description: {description}
        
        Format your response as a JSON array with the following structure for each potential CVE:
        [
            {{
                "id": "CVE-YYYY-XXXXX",
                "title": "Title of the vulnerability",
                "description": "Detailed description",
                "published": "YYYY-MM-DD",
                "cvss": {{
                    "score": X.X,
                    "vector": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H"
                }},
                "references": [
                    "https://reference1.com",
                    "https://reference2.com"
                ],
                "confidence": 0.XX,
                "match_reason": "Explanation of why this CVE matches the description"
            }}
        ]
        
        Return only the most relevant CVEs (maximum 3), ranked by confidence. 
        If you cannot find any matching CVEs, return an empty array: []
        """
        
        # Call the model and parse the response
        response = self._call_model(prompt)
        
        if not response:
            logger.warning("Failed to get enhanced vulnerability lookup from LLM.")
            return []
        
        try:
            # Parse the JSON response
            cve_matches = json.loads(response)
            logger.info(f"LLM identified {len(cve_matches)} potential CVE matches")
            return cve_matches
        except json.JSONDecodeError:
            # If that fails, try to extract JSON from the response
            try:
                # Find the first '[' and last ']' to extract the JSON array
                start_idx = response.find('[')
                end_idx = response.rfind(']')
                
                if start_idx != -1 and end_idx != -1 and start_idx < end_idx:
                    json_str = response[start_idx:end_idx+1]
                    cve_matches = json.loads(json_str)
                    logger.info(f"Successfully extracted JSON array from CVE response")
                    return cve_matches
                else:
                    logger.error("Failed to extract JSON from response. Returning empty list.")
                    return []
            except json.JSONDecodeError:
                logger.error("Failed to parse LLM response as JSON. Returning empty list.")
                return []
    
    def analyze_vulnerability(self, vuln_info: Dict) -> Dict:
        """
        Use the LLM to provide additional analysis for a vulnerability
        
        Args:
            vuln_info (dict): The vulnerability information to analyze
            
        Returns:
            dict: Additional insights and context for the vulnerability
        """
        logger.info(f"Analyzing vulnerability: {vuln_info.get('cve_id', 'Unknown')}")
        
        # Prepare vulnerability information for the prompt
        vuln_str = json.dumps(vuln_info, indent=2)
        
        prompt = f"""
        You are a cybersecurity expert. Please provide additional context and analysis for the following vulnerability:
        
        {vuln_str}
        
        Format your response as a JSON object with the following structure:
        {{
            "severity_assessment": "Critical/High/Medium/Low/Informational",
            "impact": "Detailed description of the potential impact",
            "exploitation_difficulty": "Easy/Moderate/Difficult",
            "mitigation_steps": [
                "Step 1: Specific action to take",
                "Step 2: Another specific action",
                ...
            ],
            "context": "Additional technical context that might be helpful",
            "false_positive_likelihood": "Low/Medium/High",
            "additional_notes": "Any other relevant information"
        }}
        
        Keep your analysis technical, precise, and actionable.
        """
        
        # Call the model and parse the response
        response = self._call_model(prompt)
        
        if not response:
            logger.warning("Failed to get vulnerability analysis from LLM.")
            return {}
        
        try:
            # First try to parse the entire response as JSON
            analysis = json.loads(response)
            logger.info("Successfully generated vulnerability analysis")
            return analysis
        except json.JSONDecodeError:
            # If that fails, try to extract JSON from the response
            try:
                # Find the first '{' and last '}' to extract the JSON object
                start_idx = response.find('{')
                end_idx = response.rfind('}')
                
                if start_idx != -1 and end_idx != -1 and start_idx < end_idx:
                    json_str = response[start_idx:end_idx+1]
                    analysis = json.loads(json_str)
                    logger.info("Successfully extracted JSON from response")
                    return analysis
                else:
                    logger.error("Failed to extract JSON from response. Using formatted response.")
                    # Create a structured response from the text
                    return {
                        "analysis": response[:1000],  # Truncate if too long
                        "severity_assessment": "Unknown",
                        "remediation_steps": ["Could not generate remediation steps"]
                    }
            except json.JSONDecodeError:
                logger.error("Failed to parse LLM response as JSON. Returning formatted response.")
                return {
                    "analysis": response[:1000],  # Truncate if too long
                    "severity_assessment": "Unknown",
                    "remediation_steps": ["Could not generate remediation steps"]
                }
    
    def generate_recommendations(self, vulnerabilities: List[Dict]) -> List[Dict]:
        """
        Generate remediation recommendations based on analyzed vulnerabilities
        
        Args:
            vulnerabilities (list): List of analyzed vulnerabilities
            
        Returns:
            list: List of prioritized recommendations
        """
        logger.info("Generating recommendations based on analyzed vulnerabilities")
        
        # Prepare a condensed version of vulnerabilities for the prompt
        # (to avoid hitting token limits)
        condensed_vulns = []
        for vuln in vulnerabilities:
            condensed_vuln = {
                "cve_id": vuln.get("cve_id", "Unknown"),
                "title": vuln.get("title", "Unknown"),
                "description": vuln.get("description", "")[:200],  # Truncate long descriptions
                "cvss_score": vuln.get("cvss_score", "Unknown"),
                "severity_assessment": vuln.get("severity_assessment", "Unknown"),
            }
            condensed_vulns.append(condensed_vuln)
        
        # Convert condensed vulnerabilities to a string representation for the prompt
        vulns_str = json.dumps(condensed_vulns, indent=2)
        
        prompt = f"""
        You are a cybersecurity expert. Based on the following list of analyzed vulnerabilities, 
        generate a prioritized list of recommendations for remediation.
        
        Vulnerabilities:
        {vulns_str}
        
        Format your response as a JSON array with the following structure:
        [
            {{
                "priority": 1,
                "recommendation": "Specific recommendation title",
                "description": "Detailed description of the recommendation",
                "addresses_vulnerabilities": ["CVE-YYYY-XXXXX", "CVE-YYYY-YYYYY"],
                "implementation_effort": "Low/Medium/High",
                "implementation_steps": [
                    "Step 1: Specific action to take",
                    "Step 2: Another specific action",
                    ...
                ]
            }},
            ...
        ]
        
        Focus on practical, actionable recommendations that address multiple vulnerabilities 
        where possible. Prioritize recommendations based on vulnerability severity, exploitation 
        difficulty, and implementation effort.
        
        Return a maximum of 10 recommendations, sorted by priority (highest first).
        """
        
        # Call the model and parse the response
        response = self._call_model(prompt)
        
        if not response:
            logger.warning("Failed to get recommendations from LLM.")
            return []
        
        try:
            # First try to parse the entire response as JSON
            recommendations = json.loads(response)
            logger.info(f"Generated {len(recommendations)} recommendations")
            return recommendations
        except json.JSONDecodeError:
            # If that fails, try to extract JSON from the response
            try:
                # Find the first '[' and last ']' to extract the JSON array
                start_idx = response.find('[')
                end_idx = response.rfind(']')
                
                if start_idx != -1 and end_idx != -1 and start_idx < end_idx:
                    json_str = response[start_idx:end_idx+1]
                    recommendations = json.loads(json_str)
                    logger.info(f"Successfully extracted JSON array from response")
                    return recommendations
                else:
                    # Create a single generic recommendation from the text
                    logger.error("Failed to extract JSON array from response. Creating formatted recommendation.")
                    return [{
                        "priority": 1,
                        "recommendation": "Review generated recommendations",
                        "description": response[:500],  # Truncate if too long
                        "addresses_vulnerabilities": ["All vulnerabilities"],
                        "implementation_effort": "Unknown",
                        "implementation_steps": ["Review the raw recommendations and convert to actionable steps"]
                    }]
            except json.JSONDecodeError:
                logger.error("Failed to parse LLM response as JSON. Creating formatted recommendation.")
                return [{
                    "priority": 1,
                    "recommendation": "Review generated recommendations",
                    "description": response[:500],  # Truncate if too long
                    "addresses_vulnerabilities": ["All vulnerabilities"],
                    "implementation_effort": "Unknown",
                    "implementation_steps": ["Review the raw recommendations and convert to actionable steps"]
                }]